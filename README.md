Задача: создать автоматизированный сервис, который будет предсказывать показатели OHLCV для определенной компании на следующий месяц.

Этапы работы:
1. Получение биржевых данных
2. Поднятие Docker-Compose контейнеров с БД и AirFlow
3. Создание базы данных, которая будет хранить данные за прошлые периоды и предсказанные данные
4. Выгрузка данных из API и загрузка в БД
5. Обработка данных
6. Разработка модели, которая будет предсказывать показатели
7. Проверка качества модели
8. Разработка автоматизационных процессов для модели, для автоматического создания дампов, обновления модели, получения актуальных данных.
9. Регулярная валидация модели актуальными данными
10. Автоматическое дообучение модели
11. Парсинг биржевых сайтов и финансовых сайтов для получения фундаментальных показателей
13. Дополнительная модель предсказания в долгосрочном периоде
14. Объединение двух моделей


### 1. Получение биржевых данных

На начальном этапе работы было принято решение использовать Alpha Vantage API для получения данных. Alpha Vantage предоставляет обширную информацию о финансовых рынках в реальном времени, а также исторические данные, что делает его идеальным выбором для проекта. API позволяет получить данные по акциям, валютам, сырьевым товарам и другим активам. В данном случае я буду работать с OHLCV (цены открытия, максимума, минимума, закрытия и объемы торгов), что необходимо для последующего прогнозирования.

API предоставляет возможность работать с различными временными интервалами и классами активов. Я выбрал его из-за высокой доступности, удобного интерфейса и поддержки всех основных финансовых данных, включая технические индикаторы, что позволяет использовать его как основное средство для получения данных.


### 2. Поднятие контейнеров Docker-Compose

Для удобства управления сервисами и их взаимной интеграции я решил использовать Docker-Compose. Это позволило автоматизировать разворачивание необходимых компонентов: базы данных и системы управления задачами — AirFlow. В качестве базы данных был выбран PostgreSQL, который развернут внутри контейнера, а для оркестрации задач используется Apache AirFlow, который помогает автоматизировать процессы ETL и поддерживает стабильную работу системы.

Я создал Docker-Compose файл, в котором описаны зависимости и конфигурация контейнеров. Docker позволяет изолировать каждую часть системы, обеспечивая легкость разворачивания и управления. Использование контейнеров также позволяет легко масштабировать сервисы и гарантировать их независимость друг от друга.

dockerfile

```
FROM apache/airflow:2.9.1
COPY requirements.txt requirements.txt
USER root
RUN apt-get update
RUN apt-get install -y libpq-dev gcc
USER airflow
RUN pip install -r requirements.txt
```

requirements.txt
```
numpy
pandas
SQLAlchemy
requests
psycopg2
pytrends
```

Т. к. я поднимаю Airlow, то использую шаблонный docker-compose.yaml файл, представленный на сайте Airflow. Добавим туда инициализацию собственной базы данных с хранилищем, расположенным внутри контейнера.

```
  self-database:
    image: postgres:13
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: main_storage
    restart: always
    shm_size: 256mb
    volumes:
      - self-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-d", "main_storage", "-U", "admin" ]
      interval: 10s
      retries: 5
    ports:
      - 5434:5432

  

volumes:
  postgres-db-volume:
  self-db-volume:
```
### 3. Создание базы данных

Для хранения полученных данных было решено использовать SQLAlchemy для создания структуры базы данных. Я создал две основные таблицы: первая, **OHLC**, хранит ключевые биржевые данные, такие как открытие, максимум, минимум, закрытие и объемы. Вторая таблица, **MetaDataApi**, сохраняет информацию о последних загруженных данных для каждого символа, что позволяет отслеживать, до какого момента были получены данные, и не запрашивать их повторно.

Это решение позволило организовать хранение данных таким образом, чтобы они были легко доступны для анализа и прогнозирования, а также обеспечило возможность отслеживания и обновления информации в реальном времени.


#### 4. Выгрузка из API и загрузка в БД

После того как инфраструктура была развернута, следующей задачей стало получение данных из API и их загрузка в базу данных. Были выбраны три компании: Tesla (TSLA), Nvidia (NVDA) и Apple (AAPL). С помощью библиотеки `requests` я подключилися к API и выгрузили данные с интервалом в 5 минут, что позволило собрать детализированную информацию по каждой акции.

Я разработал функцию для обработки данных в формате JSON, возвращаемом API, и их последующего преобразования в нужный формат для загрузки в базу данных. Особое внимание было уделено проверке структуры данных, чтобы убедиться в их корректности перед загрузкой. Я также внедрили механизм сохранения метаданных, чтобы в будущем не запрашивать повторно уже загруженные данные, что позволяет более эффективно использовать квоты API и ускоряет процесс обновления данных.

#### 5. Обработка данных

Для моего проекта я использую данные от AlphaVantage, которые уже обладают высоким качеством — они не содержат пропусков или выбросов. Благодаря этому обработка данных сводится к расчёту и добавлению различных технических индикаторов. Это позволяет модели лучше распознавать паттерны и тренды в финансовых временных рядах.

#### Основные метрики, которые я использую:

- **WMA (Взвешенное скользящее среднее)**:
    
    В отличие от простого скользящего среднего (SMA), WMA придаёт больший вес последним значениям в ряду. Это делает его более чувствительным к изменениям в ценах, что помогает быстрее распознавать краткосрочные тренды. Индикатор показывает среднюю цену за определённый период, акцентируя внимание на более свежих данных.
    
- **EMA (Экспоненциальное скользящее среднее)**:
    
    Подобно WMA, EMA придаёт большее значение последним данным, но в отличие от взвешенного среднего, этот вес уменьшается экспоненциально. Благодаря этому EMA более плавно реагирует на изменения, сохраняя чувствительность к недавним колебаниям рынка, что особенно важно в условиях волатильности.
    
- **RSI (Индекс относительной силы)**:
    
    RSI оценивает скорость и величину изменений цен и является одним из популярных индикаторов для определения состояний перекупленности или перепроданности актива. Значения RSI колеблются от 0 до 100. Когда RSI выше 70, это может указывать на перекупленность и возможное снижение цен, а значения ниже 30 указывают на перепроданность и потенциальный рост.
    
- **MACD (Конвергенция и дивергенция скользящих средних)**:
    
    MACD вычисляется на основе разницы между двумя экспоненциальными скользящими средними разной длины (например, 12-дневная и 26-дневная EMA). Сигнальная линия, которая обычно представляет собой 9-дневную EMA, помогает определить моменты для покупки или продажи. Когда MACD пересекает сигнальную линию, это может служить индикатором для принятия решений на рынке.
    

#### Зачем нужны дополнительные данные

На текущий момент моей модели не хватает разнообразных данных для полноценного предсказания. Чтобы улучшить прогноз, нужно дополнить датасет дополнительными финансовыми метриками. Один из вариантов — спарсить данные с сайтов, таких как Yahoo Finance, чтобы получить последние финансовые отчёты компаний. Это может значительно улучшить точность модели.

Так как финансовые данные обновляются раз в квартал, а мне требуется предсказывать цены ежедневно, я собираюсь использовать эти данные в качестве усредняющего фактора. Когда я захочу предсказать данные на следующие несколько месяцев, я буду опираться на уже существующие финансовые показатели, так как новые отчёты ещё не будут опубликованы. Это нормальная практика, однако есть несколько нюансов:

- **Фундаментальные данные** являются медленным показателем, так как они отражают долгосрочное состояние компании. Несмотря на это, такие данные помогут уловить общие тренды, но не смогут отразить краткосрочные колебания.
    
- **Использование усредняющих показателей** помогает модели учитывать текущее состояние компании на протяжении всего периода прогнозирования, но это может не дать модели понять краткосрочные рыночные изменения.
    

#### Добавление макроэкономических данных

Чтобы компенсировать статичность финансовых данных, можно дополнить модель макроэкономическими индикаторами, такими как процентные ставки или инфляция, которые меняются более часто и оказывают влияние на цены акций в краткосрочной перспективе.

#### Тренды и сезонность

Кроме финансовых и макроэкономических данных, в моей модели используются временные признаки и технические индикаторы, которые помогут уловить краткосрочные и сезонные тренды.

Таким образом, комбинация финансовых данных для долгосрочного анализа и технических индикаторов для краткосрочного прогноза обеспечит комплексное понимание рыночной ситуации.

#### Почему полезно добавлять технические метрики за разные периоды

Использование таких показателей, как EMA, за разные периоды (день, месяц, квартал) позволяет захватить как краткосрочные, так и долгосрочные рыночные тренды. Это важно для улучшения качества прогнозов:

- **Мультискейловый анализ** позволяет отслеживать тренды на разных временных интервалах.
    
- **Сглаживание шума** помогает исключить краткосрочные рыночные аномалии.
    
- **Динамика рынка** отслеживается за счёт изменений в таких показателях, как RSI или MACD, которые могут сигнализировать о смене трендов на разных временных горизонтах.
    

#### Итеративный процесс предсказания

Мой подход к прогнозированию строится на итеративном использовании данных:

1. Сначала предсказываю параметры OHLC (open, high, low, close) для текущего дня.
2. На основе этих предсказанных данных рассчитываю технические индикаторы.
3. Добавляю эти индикаторы в качестве признаков для предсказания следующих дней.
4. Повторяю процесс для всех последующих дней.

Это позволяет модели учитывать изменения трендов с течением времени, однако такой подход требует особого внимания к возможному накоплению ошибок.

#### Предотвращение каскадного эффекта ошибок

При использовании итеративного процесса важно учитывать накопление ошибок. Чтобы минимизировать этот эффект, я планирую использовать методы сглаживания, такие как фильтры Калмана, или другие подходы для уменьшения влияния ошибок на последующие прогнозы.

#### Лаговые признаки и их роль

Создание лаговых признаков, таких как last_ema, last_rsi, last_macd и last_{ohlc}, позволит модели учитывать предыдущие значения индикаторов и OHLC. Это стандартный подход для временных рядов, так как он помогает уловить автокорреляцию данных — важную характеристику для прогнозирования финансовых временных рядов.

Использование временных и технических индикаторов, дополненных финансовыми данными, создаёт комплексную модель, способную предсказывать как краткосрочные, так и долгосрочные изменения цен. Дополнение лаговыми признаками и другими временными характеристиками обеспечит необходимую контекстуальную глубину для точных прогнозов.

### 6. Создание модели

Итеративный процесс предсказания — это метод, при котором модель на каждом шаге предсказывает значение, основываясь на предыдущих предсказаниях. Этот подход особенно эффективен при работе с временными рядами или последовательными данными, где каждое новое значение зависит от предыдущих. Применение такого метода позволяет моделям "заглядывать" в будущее, постепенно корректируя свои прогнозы на основе ранее предсказанных данных.

### Реализация итеративного процесса:

1. **Обучение модели**:
    
    - Модель обучается на данных, где значения будущих шагов напрямую зависят от предыдущих. Например, если речь идёт о временных рядах, данные могут содержать метрики за несколько предыдущих дней, а модель учится прогнозировать следующее значение на их основе.
2. **Предсказание одного шага**:
    
    - В процессе тестирования или использования модели в реальном времени она делает прогноз для одного шага вперёд, используя доступные данные текущего шага.
3. **Использование предсказания для следующих шагов**:
    
    - Предсказанное значение используется для прогнозирования следующего шага. Этот процесс повторяется итеративно, шаг за шагом.
4. **Повторение предсказаний**:
    
    - Процесс продолжается до тех пор, пока не будут сделаны все необходимые прогнозы. Каждый новый шаг зависит от предыдущих предсказаний.

### Пример использования:

- **Временные ряды**: Допустим, я хочу предсказать цены акций на ближайшие 10 дней. Модель сначала делает прогноз для 1-го дня, затем это значение используется для предсказания на 2-й день, и так продолжается до 10-го дня.

### Основные модели для итеративного предсказания:

1. **ARIMA (AutoRegressive Integrated Moving Average)**:
    
    - Модель временных рядов, которая учитывает автокорреляцию данных. ARIMA делает итеративные предсказания, используя предыдущие значения и ошибки для более точных прогнозов.
2. **RNN (Recurrent Neural Networks) и LSTM (Long Short-Term Memory)**:
    
    - Эти рекуррентные нейронные сети учитывают зависимость текущего предсказания от предыдущих шагов. LSTM особенно эффективна для длинных последовательностей, так как она справляется с проблемами, связанными с затухающими градиентами в RNN.
3. **Градиентный бустинг (LightGBM, XGBoost)**:
    
    - Хотя этот метод обычно не используется для итеративных предсказаний напрямую, его можно адаптировать для работы с временными рядами. На каждом шаге можно обновлять данные, подаваемые на вход модели, что позволяет предсказывать значения поэтапно.
4. **SARIMA (Seasonal ARIMA)**:
    
    - Модификация ARIMA, которая учитывает сезонные колебания во временных рядах. SARIMA использует сезонные и трендовые компоненты для улучшения точности итеративных прогнозов.
5. **DeepAR** (Amazon SageMaker):
    
    - Глубокая рекуррентная модель, специально разработанная для временных рядов. Она обучена на множестве различных временных рядов и делает итеративные предсказания на основе предыдущих шагов.

### Ключевые моменты:

- **Накопление ошибок**: Одной из проблем при итеративных предсказаниях является накопление ошибок. Поскольку модель использует свои собственные предсказания на каждом шаге, ошибка может накапливаться и увеличиваться со временем.
    
- **Устойчивость моделей**: Некоторые модели, такие как LSTM, специально разработаны для смягчения эффекта накопления ошибок за счёт механизмов памяти.
    

Итеративное предсказание может быть реализовано с использованием различных методов, таких как ARIMA, SARIMA, LSTM и градиентный бустинг. Основная задача — правильно организовать процесс, чтобы каждое следующее предсказание основывалось на предыдущих значениях, учитывая при этом возможное накопление ошибок.

#### 6.1 Модели типа ARIMA

Модели ARIMA широко применяются для анализа временных рядов, так как они могут предсказывать значения, опираясь на автокорреляцию и тренды. Эти модели могут быть эффективно использованы для итеративных прогнозов, особенно если временные ряды демонстрируют сезонные или трендовые колебания.

В процессе реализации

### 6.2 Модели LSTM

LSTM — это усовершенствованная версия рекуррентных нейронных сетей, которая позволяет учитывать долгосрочные зависимости между шагами. Они могут быть использованы для предсказания значений во временных рядах, где текущие данные сильно зависят от предыдущих.

Кратко модели LSTM можно представить как на следующем изображении:

$$f(t) = \sigma(W_f * [h_{t-1}, x_t] + b_f)$$
$$i_t = \sigma(W_i * [h_{t-1}, x_t] + b_i)$$
$$\widetilde{C_t} = \tanh{(W_C * [h_t-1, x_t])}$$
$$C_t = f_t * C_{t-1} + \widetilde{C_t}$$
$$o_t = \sigma(W_o * [x_t, h_{t-1}])$$
$$h_t = o_t * \tanh(C_t)$$

После того как данные подготовлены, рассчитаны технические метрики и выделена дата в качестве признака, следующим шагом является разделение данных на тренировочные и тестовые выборки.

Как это сделать?

Пусть у нас есть датасет размером $m \times n$.

Здесь используется метод _lookback window_. Я выбираю определённый период, пусть это будет $n$, и, учитывая этот период, делим наш датасет на тренировочную и тестовую выборки. То есть датасет разбивается на $\frac{m}{n}$ или $m-n$ выборок, где от 0 до $n - 1$ — тренировочные значения, а $n$ — тестовое значение.

Если логически предположить, существует два подхода:

1. Разбить датасет с неповторяющимися элементами (Overlapping).
2. Разбить датасет с повторяющимися элементами (_Without overlapping_).

В первом случае я разбиваю датасет на $n$ выборок и не используем соседние элементы.  
Во втором случае я разбиваю датасет на $m-n$ выборок, и у нас присутствуют повторяющиеся элементы.

Стоит ли использовать второй случай с повторяющимися элементами? Ответ — да, это даст больше тренировочных значений.


Создаем torch.Dataset

```
from torch.utils.data import Dataset

class TSDataset(Dataset):
    def __init__(self, X: pd.DataFrame, lookback: int = 14):
        # assert  X.shape[0] == y.shape[0], "X and y must have the same number of lines"
        self.X = X.iloc[lookback:, :].to_numpy('float32')
        self.y = X.shift(-lookback).iloc[:-lookback, :5].to_numpy('float32')
        self.X = torch.from_numpy(self.X)
        self.y = torch.from_numpy(self.y)
        self.lookback = lookback

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, index):
        return self.X[index], self.y[index]
```

Создаем DataLoader

```
from torch.utils.data import DataLoader

  

batch_size = 32


train_dataloader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True
)

  

for i, (X_batch, y_batch) in enumerate(train_dataloader):
    print(f'Batch {i}: X_batch: {X_batch.shape}, y_batch {y_batch.shape}')
```


Первая простейшая модель:
```
class Kleopatra(nn.Module):

    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=21, hidden_size=100, num_layers=2, batch_first=True)
        self.linear = nn.Linear(100, 5)

    def forward(self, X):
        X, _ = self.lstm(X)
        X = self.linear(X)
        return X
```

Обучение первой модели

```
optim = torch.optim.Adam(model.parameters())
loss_fn = torch.nn.MSELoss()

n_epochs = 200
for epoch in range(n_epochs+1):
    model.train()
    for X_batch, y_batch in train_dataloader:
        y_pred = model(X_batch)
        loss = loss_fn(y_pred, y_batch)
        optim.zero_grad()
        loss.backward()
        optim.step()
    if epoch != 100:
        continue
    model.eval()
    with torch.no_grad():
        y_pred = model(train_dataset.X)
        train_mse = loss_fn(y_pred, train_dataset.y)
        y_pred = model(test_dataset.X)
        test_mse = loss_fn(y_pred, test_dataset.y)

    print("Epoch %d: train MSE %.4f, test MSE %.4f" % (epoch, train_mse, test_mse))
```

Я обучил модель и сделал предсказание на основе лага со степенью 1. Теперь необходимо создать метод, позволяющий модели предсказывать дальше, используя только те значения, которые она получила на предыдущем шаге предсказания. Стоит отметить, что это может привести к накапливающейся ошибке.

Необходимо создать _pipeline_ для последовательной обработки ряда после предсказания — по сути, это тот же _preprocessing_pipeline_, только без _scalers_.

Для предсказания нам потребуется:

1. **Дата для конвертации**: необходимо создать список будущих дат, по которым я буду делать предсказания, с возможностью последовательно обновлять дату в методе `predict`.
2. **Метод для расчета технических метрик на основе предыдущих значений**: этот метод должен опираться на дату. То есть нужен временный массив, который "объединит" новые предсказанные значения с предыдущими для расчета метрик.
3. **Временной массив в методе `predict`**: в этом массиве будут храниться предсказанные даты.

Я остановился на этом этапе и продолжу писать проект дальше.

